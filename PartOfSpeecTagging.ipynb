{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Aim \n",
        "\n",
        "The aim of this module is to implement the part of speech tagging (POS) mechanism to identify the type of speech associated with a particular word. Tagging can be tricky since some words might appear in different context and hence be associated with different part of speech.\n",
        "\n",
        "* The whole team played well. [adverb]\n",
        "* You are doing well for yourself. [adjective]\n",
        "* Well, this assignment took me forever to complete. [interjection]\n",
        "* The well is dry. [noun]* \n",
        "Tears were beginning to well in her eyes. [verb]\n",
        "\n",
        "\n",
        "During the course of this module we would be learning :\n",
        "* Learn how parts-of-speech tagging works\n",
        "* Compute the transition matrix A in a Hidden Markov Model\n",
        "* Compute the emission matrix B in a Hidden Markov Model\n",
        "* Compute the Viterbi algorithm\n",
        "* Compute the accuracy of your own model\n",
        "\n",
        "## Data Source\n",
        "* The tagged training data has been preprocessed to form a vocabulary (hmm_vocab.txt).\n",
        "* The words in the vocabulary are words from the training set that were used two or more times.\n",
        "* The vocabulary is augmented with a set of 'unknown word tokens', described below.\n",
        "\n",
        "When POS tagger would encounter a missing word:\n",
        "* To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag.\n",
        "* For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'.\n",
        "* A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n",
        "\n"
      ],
      "metadata": {
        "id": "-HI-BZ60XzCX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_BlQd6PWWpW"
      },
      "outputs": [],
      "source": [
        "# Importing packages and loading in the data set \n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the drive to access the relevant data \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ19CcqGoP7g",
        "outputId": "66086d02-f0f2-4e17-9da8-9be2942e1ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/My Drive/WSJ_02-21.pos\", 'r') as f:\n",
        "    training_corpus = f.readlines()\n",
        "\n",
        "print(f\"A few items of the training corpus list\")\n",
        "print(training_corpus[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiAI-HH6qBiB",
        "outputId": "df781a5b-01a6-4076-c4c4-14afc3ed0b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few items of the training corpus list\n",
            "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/My Drive/hmm_vocab.txt\", 'r') as f:\n",
        "    voc_l = f.read().split('\\n')\n",
        "\n",
        "print(\"A few items of the vocabulary list\")\n",
        "print(voc_l[0:50])\n",
        "print()\n",
        "print(\"A few items at the end of the vocabulary list\")\n",
        "print(voc_l[-50:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UMGel0EqN3m",
        "outputId": "29b970b0-d6d4-44a0-dddd-7fa2a0c9461b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few items of the vocabulary list\n",
            "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
            "\n",
            "A few items at the end of the vocabulary list\n",
            "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {} \n",
        "\n",
        "# Get the index of the corresponding words. \n",
        "for i, word in enumerate(sorted(voc_l)): \n",
        "    vocab[word] = i       \n",
        "    \n",
        "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
        "cnt = 0\n",
        "for k,v in vocab.items():\n",
        "    print(f\"{k}:{v}\")\n",
        "    cnt += 1\n",
        "    if cnt > 20:\n",
        "        break\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etiJFEwdrCmn",
        "outputId": "4206f9ee-197f-481c-886b-b35c948f43c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary dictionary, key is the word, value is a unique integer\n",
            ":0\n",
            "!:1\n",
            "#:2\n",
            "$:3\n",
            "%:4\n",
            "&:5\n",
            "':6\n",
            "'':7\n",
            "'40s:8\n",
            "'60s:9\n",
            "'70s:10\n",
            "'80s:11\n",
            "'86:12\n",
            "'90s:13\n",
            "'N:14\n",
            "'S:15\n",
            "'d:16\n",
            "'em:17\n",
            "'ll:18\n",
            "'m:19\n",
            "'n':20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load in the test corpus\n",
        "with open(\"/content/gdrive/My Drive/WSJ_24.pos\", 'r') as f:\n",
        "    y = f.readlines()\n",
        "\n",
        "print(\"A sample of the test corpus\")\n",
        "print(y[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOqFkBxk-xSL",
        "outputId": "0b287eb1-6402-44f8-cad1-8cb26e6bf9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sample of the test corpus\n",
            "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Punctuation characters\n",
        "punct = set(string.punctuation)\n",
        "\n",
        "# Morphology rules used to assign unknown word tokens\n",
        "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "adv_suffix = [\"ward\", \"wards\", \"wise\"]"
      ],
      "metadata": {
        "id": "j0KQyouRC9eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some utility functions \n",
        "def get_word_tag(line, vocab): \n",
        "    if not line.split():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "        return word, tag\n",
        "    else:\n",
        "        word, tag = line.split()\n",
        "        if word not in vocab: \n",
        "            # Handle unknown words\n",
        "            word = assign_unk(word)\n",
        "        return word, tag\n",
        "    return None \n",
        "\n",
        "def preprocess(vocab, data_fp):\n",
        "    \"\"\"\n",
        "    Preprocess data\n",
        "    \"\"\"\n",
        "    orig = []\n",
        "    prep = []\n",
        "\n",
        "    # Read data\n",
        "    with open(data_fp, \"r\") as data_file:\n",
        "\n",
        "        for cnt, word in enumerate(data_file):\n",
        "\n",
        "            # End of sentence\n",
        "            if not word.split():\n",
        "                orig.append(word.strip())\n",
        "                word = \"--n--\"\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            # Handle unknown words\n",
        "            elif word.strip() not in vocab:\n",
        "                orig.append(word.strip())\n",
        "                word = assign_unk(word)\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                orig.append(word.strip())\n",
        "                prep.append(word.strip())\n",
        "\n",
        "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
        "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
        "\n",
        "    return orig, prep\n",
        "\n",
        "def assign_unk(tok):\n",
        "    \"\"\"\n",
        "    Assign unknown word tokens\n",
        "    \"\"\"\n",
        "    # Digits\n",
        "    if any(char.isdigit() for char in tok):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Punctuation\n",
        "    elif any(char in punct for char in tok):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Upper-case\n",
        "    elif any(char.isupper() for char in tok):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Nouns\n",
        "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Verbs\n",
        "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Adjectives\n",
        "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Adverbs\n",
        "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "\n",
        "    return \"--unk--\"\n"
      ],
      "metadata": {
        "id": "CmO7hNyHbxFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus without tags, preprocessed\n",
        "_, prep = preprocess(vocab, \"/content/gdrive/My Drive/test.words\")     \n",
        "\n",
        "print('The length of the preprocessed test corpus: ', len(prep))\n",
        "print('This is a sample of the test_corpus: ')\n",
        "print(prep[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK_F-tR2_Ylq",
        "outputId": "f339c83e-6094-4ae0-ffe1-88e07f8dfdd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the preprocessed test corpus:  34199\n",
            "This is a sample of the test_corpus: \n",
            "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of speech tagging \n",
        "\n",
        "There are some words which have multiple tags and hence are ambiguous , \n",
        "In the training .\n",
        "* the word is a verb and it is not ambiguous.\n",
        "* In the WSJ corpus, 86% corpus is unambigous . \n",
        "* About 14% are ambigious .\n",
        "\n",
        "\n",
        "### Calculating some dictionaries for future computations .\n",
        "\n",
        "* Transition counts : This dictionary is used to calculate the frequency of the instances from one word to another . P(t(x)/t(x-1)) indicates trasition from one state t(x-1) to another t(x) . In this scenario we have the following cases - \n",
        "  * Key : (prev_tag, tag) , value : # two tags appeared in that order.\n",
        "\n",
        "### Emmission count:\n",
        "This dict would indicate the frequency of instances from one state to a word . The probability is P(w(x)/t(x)) . Here :\n",
        "  * Key : (tag, word) , value : # the word occured with that POS tag.\n",
        "\n",
        "### Tag Count \n",
        "Key : tags , value: # a tag appeared in the dictionary .\n"
      ],
      "metadata": {
        "id": "nMKk5yReah6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dictionaries(training_corpus, vocab):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        training_corpus: a corpus where each line has a word followed by its tag.\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output: \n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
        "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize the dictionaries using defaultdict\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    \n",
        "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
        "    prev_tag = '--s--' \n",
        "    \n",
        "    # use 'i' to track the line number in the corpus\n",
        "    i = 0 \n",
        "    \n",
        "    # Each item in the training corpus contains a word and its POS tag\n",
        "    # Go through each word and its tag in the training corpus\n",
        "    for word_tag in training_corpus:\n",
        "        \n",
        "        # Increment the word_tag count\n",
        "        i += 1\n",
        "        \n",
        "        # Every 50,000 words, print the word count\n",
        "        if i % 50000 == 0:\n",
        "            print(f\"word count = {i}\")\n",
        "            \n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
        "        word, tag = get_word_tag(word_tag, vocab)\n",
        "        \n",
        "        # Increment the transition count for the previous word and tag\n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        \n",
        "        # Increment the emission count for the tag and word\n",
        "        emission_counts[(tag, word)] += 1\n",
        "\n",
        "        # Increment the tag count\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
        "        prev_tag = tag \n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return emission_counts, transition_counts, tag_counts"
      ],
      "metadata": {
        "id": "1wFu_WQ1aBfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n",
        "# get the POS states \n",
        "states = sorted(tag_counts.keys())\n",
        "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
        "print(\"View these POS tags (states)\")\n",
        "print(states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDcZZP62iFG8",
        "outputId": "afe4059f-e6e4-4185-ff80-7bebab10cf55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word count = 50000\n",
            "word count = 100000\n",
            "word count = 150000\n",
            "word count = 200000\n",
            "word count = 250000\n",
            "word count = 300000\n",
            "word count = 350000\n",
            "word count = 400000\n",
            "word count = 450000\n",
            "word count = 500000\n",
            "word count = 550000\n",
            "word count = 600000\n",
            "word count = 650000\n",
            "word count = 700000\n",
            "word count = 750000\n",
            "word count = 800000\n",
            "word count = 850000\n",
            "word count = 900000\n",
            "word count = 950000\n",
            "Number of POS tags (number of 'states'): 46\n",
            "View these POS tags (states)\n",
            "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"transition examples: \")\n",
        "for ex in list(transition_counts.items())[:3]:\n",
        "    print(ex)\n",
        "print()\n",
        "\n",
        "print(\"emission examples: \")\n",
        "for ex in list(emission_counts.items())[200:203]:\n",
        "    print (ex)\n",
        "print()\n",
        "\n",
        "print(\"ambiguous word example: \")\n",
        "for tup,cnt in emission_counts.items():\n",
        "    if tup[1] == 'back': print (tup, cnt) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ_kEmNQigcT",
        "outputId": "f98ba4f5-f230-498c-871c-24953809546e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transition examples: \n",
            "(('--s--', 'IN'), 5050)\n",
            "(('IN', 'DT'), 32364)\n",
            "(('DT', 'NNP'), 9044)\n",
            "\n",
            "emission examples: \n",
            "(('DT', 'any'), 721)\n",
            "(('NN', 'decrease'), 7)\n",
            "(('NN', 'insider-trading'), 5)\n",
            "\n",
            "ambiguous word example: \n",
            "('RB', 'back') 304\n",
            "('VB', 'back') 20\n",
            "('RP', 'back') 84\n",
            "('JJ', 'back') 25\n",
            "('NN', 'back') 29\n",
            "('VBP', 'back') 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing \n",
        "\n",
        "* first we will assign POS tag to every word . \n",
        "* Determine what percentage was correct .\n"
      ],
      "metadata": {
        "id": "fFlaNxf7l8Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_pos(prep, y, emission_counts, vocab, states):\n",
        "    '''\n",
        "    Input: \n",
        "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
        "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
        "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "        states: a sorted list of all possible tags for this assignment\n",
        "    Output: \n",
        "        accuracy: Number of times you classified a word correctly\n",
        "    '''\n",
        "    \n",
        "    # Initialize the number of correct predictions to zero\n",
        "    num_correct = 0\n",
        "    \n",
        "    # Get the (tag, word) tuples, stored as a set\n",
        "    all_words = set(emission_counts.keys())\n",
        "    \n",
        "    # Get the number of (word, POS) tuples in the corpus 'y'\n",
        "    total = len(y)\n",
        "    for word, y_tup in zip(prep, y): \n",
        "\n",
        "        # Split the (word, POS) string into a list of two items\n",
        "        y_tup_l = y_tup.split()\n",
        "        \n",
        "        # Verify that y_tup contain both word and POS\n",
        "        if len(y_tup_l) == 2:\n",
        "            \n",
        "            # Set the true POS label for this word\n",
        "            true_label = y_tup_l[1]\n",
        "\n",
        "        else:\n",
        "            # If the y_tup didn't contain word and POS, go to next word\n",
        "            continue\n",
        "    \n",
        "        count_final = 0\n",
        "        pos_final = ''\n",
        "        \n",
        "        # If the word is in the vocabulary...\n",
        "        if word in vocab:\n",
        "            for pos in states:\n",
        "\n",
        "            ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "                        \n",
        "                # define the key as the tuple containing the POS and word\n",
        "                key = (pos,word)\n",
        "\n",
        "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
        "                if key in emission_counts.keys(): # complete this line\n",
        "\n",
        "                # get the emission count of the (pos,word) tuple \n",
        "                    count = emission_counts.get(key,0)\n",
        "\n",
        "                    # keep track of the POS with the largest count\n",
        "                    if count_final < count: # complete this line\n",
        "\n",
        "                        # update the final count (largest count)\n",
        "                        count_final = count\n",
        "\n",
        "                        # update the final POS\n",
        "                        pos_final = pos\n",
        "\n",
        "            # If the final POS (with the largest count) matches the true POS:\n",
        "            if pos_final == y_tup[1]: # complete this line\n",
        "                \n",
        "                # Update the number of correct predictions\n",
        "                num_correct += 1\n",
        "            \n",
        "    ### END CODE HERE ###\n",
        "    accuracy = num_correct / total\n",
        "    \n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Wwc47MPTngUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
        "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTNOZnSSonsQ",
        "outputId": "a2ed57a9-f1d8-44f2-8059-b2e59ab44234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of prediction using predict_pos is 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hidden Markov Model for POS \n",
        "\n",
        "* We would be using the Viterbi decoder to implement the Hidden Markov Model \n",
        "\n",
        "HMM contains the probability tranisitons between different states : \n",
        "* In this case, the states are the parts-of-speech.\n",
        "* A Markov Model utilizes a transition matrix, A.\n",
        "* A Hidden Markov Model adds an observation or emission matrix B which * describes the probability of a visible observation when we are in a particular state.\n",
        "* In this case, the emissions are the words in the corpus The state, which is hidden, is the POS tag of that word.\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPwAAAA9CAYAAACJHwR2AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAABVbSURBVHhe7Z0LTFtXmsf/ux2JKLvyaCoZTUdYKQ3TqrjTDc4GZMQ2Rplg1AZoAut0YpnZUGhDyWxIrCwseTC0iUua0DKRKbOBMi0O28bDTgrZZnGmEc4W2ZtJcaIEM0pjoiRmJ5EtbYWlRlyp0d1zrq8Bm+snjwC+P8W5vseP+8Df+R7nfN/5K5YAERGRpOCv+a2IiEgSIAq8iEgSIQq8iEgSIQq8SJwwcH1WC+1xO3x8y0qBudoGrd4M10O+YQUiCnwy860L1k8aUV26Ec888wz32FjRCNPXVJR9sB8vg2GI8b+Xx32uDtoLuWjer4SEb1sppGTVoHnDALSH+uHm21YcNEovkmxMsMP/tot9KT2dfammgx1w3mMnv+df8gyyJ7aVsrtqStn09F1sn5tvp7j72F3ppWyHk98PYfJ6B7tLt4s9c5NvWDJ42IGDO9gd7w2SK4/GCNuxLZ3d+ek9fn9lIWr4ZOOhE51Vm1HW7IWq/RIuGSuhzpQh5Qn+dakK+ncKMX7eAWSux9o0vp2Y8vbTR2Epq4Imk28KgoHjCwMsQ8Cq1XzTUuHuILp77LD/QBKDVSKH5g0NrB90wvot3xQvN9qQo7fAy+8uJUSBTyrc6D9UAcNFoPg3H6BJLePbQ8hUQEU2UrWC/Px57vah45QXlcWqMELjhOMs2QR1EksD7/VB2MlW92KGvyEKElUJKr0mdF9M0LD/nhyTCXaFlgqiwCcRzq69qD1L9M7LB3CgKIyw86wij8LMaQFxXxmAFTrkvpjCt4Qw6sAAVWl5csQmVouFD84/WchWCXlGjFGHFDlytYC1377ifHlR4JOFu2a0HCFmOqTQv15M/o+EFPL6ZpQoAgLixajdSrS3DNIQmXG25/gDflsMRMcTTmnxPNnPaef2Hh9eC2q5QOQ6VPTQBjvq8+l+LSxRbW0JpOnEthlyYuxxDEV47TAdqoZWqyWPAhSU1qOTC6TOA7wvL7LCGfloC5uens6mZxvZYb4tdoZZI/3svgHWw7cEM8EOHiSvp+9gz9zhm5YKzg52Cz33d23sJN8UC54v9pDryWaN1/mGeHAY2fSavjD3KjITX51gS8lx93x6ayrAeO/TneRcSsm50Cu4x57Zmfh9FjV8UuDFmIPXuFtn+OWx4vPBQ7dPS8NYBi44L5CNNBcZa/wtQfjccFx1I2Gv1ueC5RMLXAl8gXdsmLM81PIMCDkjvrsOOMZnf7H0qZ+S/73weBZRxY92oqK8De43WnHstYypWIks71Wo4UCL2U7O144BTz7kQvc5BkSBTwrccJ/3P1M/lyb4w5+GgfeGHa6ZEeqHvMCH464LNmomFwj7765z9Sg70gn7A74hVm6YoK2qgFajRXWTDQJyGQUGLif13+VYv1aoq3Khr6EMho/sYSPqnofhD+q9aIC2nJjdoY8jZuCKEbVCr+0xk6MK4Yb5hIGItRxVxcrgv1Famv++XhiGqd+ElDdL4u+0A/CaXmRFQ8xAHTW509ld/VEMzUkbe0J1grXNtH/v0/F38vmTws6A3/yN4bsTxNO/i3z/YXYw+iB6CCOsMZucdyJuDDXLE72mREz6wD0WPFfepaKPbR3kqhJH1PBJgQwZG/wabvx+RF0NZ08LnG9qoJypYlZLkMo/FcJ9s5/8r0T+i7wW9TqDLYTHxfgYRqjqnnJjfHCPxudapK6ObA/NG/fdoLZIZJdLAf2vdYlrd4Io8EmColjPja07+wfgCDNX3H2xEQZnOZpeCxmyk0iRSmX5jlfA9HXCcZFspvx3YprW1cN6nzx96EDnvzSiviIHFb0LMMD1yAfXGBHgR/x+CL6bw5wQBfx3ZsiIso9HOYFnrnai/lA9KnIqYL5L3x2M9/4tbpv6ZIxDeXPlSSnpMgl/kzLb5ZpyqfKh/NncOiBR4JOFNRo0deigGG1DdZUB/aPTwSjmWyf6j1Sg0Z6L5veKiT0QigzyArK56Rb05RnaC2yQcZ9zn21BX1YTdJmkKzhtBPMLDdFLXvi8vsSDdoL4YP31OhRs3oiX3rcLfjfDTHDbF9JIb+UjnU+7Bwf+WQ0J6aRMHzLQ/EJBJNtH/s3+tOfBGPm/GBmLNYloTT7Kt5LztNjhnOrAGHjtnaje1gG3kva4t+B94Ib9EzMcCcYSF6HiDU3CqMaZ9GYcK5NFCRiFgY6pbnGh6nLNnMyZueNGv74et7a2Q5+3SD3/fEPHeE92o2/IAgfVbGsUUOcV4lUimOrM8Nfk/kyLjQ0Z6LrWBFXI27xWA2rrLGCIYEnVDTj2hoKLMDM+8qu8a0ZZiQ26P3ZBs5Y0PrCi5eRA2CAZRZJXg4aXp7sd77lq5OxJDTk2A2e7FkXH6dwCPXpv15COJQSGCLb+LRhdUsh+okD5AT2K13K6ng48kGsqQ9H/6HChSxMSbCSdyaF1qPjfZlwir83uAKNwtQ3PfCTDZWO0+Q4hPHSh/6QBRqsP0lR6nqSjLS5BVZESUq8Vhro6WP4iQ+Y/NaH1l/LEZInz5CPxfwNsXSBgEOaRrd7JHv54WCAxYZIdPlnKZsc5BjoLLqCRyPjxAvCdjT2aXcoaHXO6ouXHnTPsTvK3PvpVPNc9ydrey2az9w2w9yYnphN04iRi0O67QfZEtjH+QBYNTmZns3u+uMdOToRcE3ntKLnWnb9PMIFmDuPwC010k/5HajTfvo3b5hr/fmYDztH9wOPWZRzb5IOpqQybQxIGmK+NaDwtx7E3Q4YZOEgP3VUN7e5wwxQx8sCCxnItWkivuCisVmK3QY7uXS2wr+C86VmsKUS5VorOfivRfzHis2OgPRUlW9di9Egt+u7w7fPJHResBfK4tbBvaABt0hJszxiFQd8X9Bv0WfvQKa1E1QwrIy6yKnGNuEZxafdFImYf3nnd6n+yKSSK+IQUKl051OSp92w3BqcCIG70fdgGvK6B6kd800wYB/qOWGB/tCox04THbe2GacgO/O3imdiSTRpUSTvR8tljnj66qEigqtRD1dsB8yjfFA2JFBlKDxxdnbD9nR4l1KSPB2rGks68to2a7QMwVGuh/WCmv05M709NUL0cf26+5KkMKL0OdHTaIN9bMsOkd8J8ygx1vQ7KhLP+UiBZahmDAXhNHwUPO7DPb74f/lLAriLm3g7evDc6/E2TthNsdqSplsTsySbv3/JRDMZYWJPew/bV0OMmMkY7N+59uoNNzw4Zr04C7vXvYbN3nmGXQrb45OBRdvN7c3QXQ6DTWLP39S2J61sIYtPwjAvDNPUxTMaRz+Xk0g8BHeRcL87AMWSGNzP8FEDnVX/gRvnsHHKrfE4M0xlkeXKsXTwFzyF7kVybtw2DV+Y39rzUkRUdQ0+BDfVLoMRViqoBF/YLuYuJQUtc1V8pRM87QiMVK4PYBP4bYn7TrTRXQIB9sFtM3DOVoZKPorrg6CXinJURcuOcaMvxl1IqOuI3hzvLnyf7OWi7we3GhPd8rT9Da10FuCMP1WMj3V/MogNpGVxU2P7NnCIQy5AUZLzWip4VWuKqp0WDjKVqjs8DMQm822nzC9JWRfDwxSMv7O21ONwrhXJ/L1oDEzYe+Gc4SX8iDflRyFFz+TZuX+sitgAhrxmXuODfZdT8jHtDTEhfbuUChucO+qMJld1/9gcQW9SLFyjhJ6M4neOL18mIiMyRGMbh+TFJmlO8RgGlLGBAMRh3A5l5JdheWQLVmhmiPdqJoi0GpP3mMtqLBESQmE45pS3A3l5c/tWs0VNySDccY6R7yOLH7R/0ozrXjaqgsVYv+nfnoPa8Gq22dhT/mG8OQDoj558mIFNOZx3Fhw8uSx/G0jVQPytkNPLHv9OAc/9ZGWF+AAP7BxUwXuV3YyVPj643FPNmroqIUKILPGOH4XktOonu1PfFqInpxAMi0OowAu+fxGGHruMamjbNFkdXjxYFf8hAV1sTVFSQhQQ+cF50mFBA4BirAS/VubD7d+3QZcYjNk6YylswiHE4h1woDHOO0x1OmEkfiwB1a0REIkEt3yCowEckUEAg/WjsEemImUaBiP8utu8+3xQNoSj9dX+UPztMBtec4bOXBEclOAIjBEtkQpCISAxE9eHd1weJziNocyGPVVGmpEQwcd0YoRH/vHxk8Wa4d9QVd8TX7RrhfOeSLP5IxA1wxp8wPXeI9RD5tlCTPiQnOpbHKTr2LCIyz/CCH4ZA6aJ0dkc8dbo9A+weqn0/FBhj5y2GKc1Mp2y+wuf4fjfMduw/zNbtzA6e1jhLwwfOK2AlTLK2d7PZugGqje+xAwfr2Lp9W9iX5jJGG1XD87nWS3QKpYiIEJE1POOEjSsAKEWuPI6RSWkaXsgkmvsv3tma+xHDZVwpnibf98iN/pN9WP+OP8c39uwqBgw3rfUFpBErwfd1J4zeA6hRS+CzdOK/sitR8mMPJh/44rYcYsbnhYeORMjXLskplCIiQggH7W50YmOJgRjfoejQNdIEVdRxSmLGNj8P7ZBQQM0L65Fa1F1kIHtSisJ/PYbKv/cHxQSzqygCQTtm1ITaGiPGnpQhLascDfuK/eOnD0lH4bOjMdcISXcPGrisNgbOzxphus59NAwZ0ByshCJwbdwxa5EaLmjHjUR0QNXz39AHVYtY/jBjZtQddmO7UQ+l0LToRWEFZCYuRTg9vwBEnVorSJjsqrBTa4W59/udbPo2IzsyMcmGJkLFTBSTfllMrf1+gr012M0erinllpWirlm6ajqzkauQGprJ6O5j92TvTLgq6rySrJmJMxHIVp2VxXezmy0NeQ/9bQ4LZCfGNtMuAVKUhajKssNkiSPBZF6yq5ywfGyF6h/VwGfVMCZaKSAi5Bin7VDtDSkFtYSgbk71z9ehoMkGWVETTo/w2Y1fHkO+x4CK3dVchVRpUFFLolWbazH+ph4aoSnRYmbi4hPIVv0P/ZTraP3Y4g+kB3hWh15uIhqxf1XEqr5G3n9ZD0Vg+bAZLJjA01l1ur2V8Hxkjn2NrrlmV3HQ+m0yeM8ZYGKqoFPGaw5Sl0MLrd4ILkfreDW05cE/ON9FMzpQg93FccQ1Fg2adlyBzRoDvKp2XPqyHZVqOWQBV4VmN+5vQuEdC1chdf1z09fADJlw9LwGVVuFx1iWZ2aiFxZ9fFO3lyI09wRaHTR0Z7QDA/bQCBcDn8cBdZEa8kh/Hl7TLxBiAYzFhstmIyZdtIyv4fep2TfzntIFDkhb2L/Vcs1M9J93IItzeeK/hqNfTXAuL2ey7x8IKTgzwna8siXsyr4BFlDDU1Kg+FUXWp8woq53DgsRPJGCFGLPPH7rmS7GaERKSxdqspagLT/aib17+olOU+PA/igZXz8gj5l15OkCB1ZAtyFM6SQxM/Hxwbgwcr6YWGMSKF+p8gfBez+HdWad//ExDI+qoBBc2XeaBRZ4CjnJ/T1oTbSeHUWqRutjr2dHkaG4pWeJRo0DCxmQ27W3anZuQQjSzAY0b/PXnqN4nTZYyR2WPRV8bWJmIs2paEM9XeetXIuywgJUNPfDFW9MwWdFY04OGhOJf9Bs1bxcZFInPlMNHS0/DAs+H5oeR+Mq9GrXh9Tmm80iCLzIojBqgYkrSiRFeV70mf2yTZXQZE0Lt3uM1pZfC1nIpIKkzkx86IJZvxkFpyZR2NqDnu4e9P6hCRlna6E95O/wGHsLcpqFq+ZS6FDz1Gte8gm+Im1QexRotio2BFLNZSjcznnyQcE71+gAlPK1URPFRIFfIQTWUANKopp1s/HBx9Wf/imkglIcWJtOjRcyBOw0mploj3969DRUi5pg+SacCKRCtoFswpTJXhh8sL6rRf1ZOZrf10MVuC+rlSh8jdyRs0b0jfoLvRSGc4O+taBx3ToU7DaRv41/MY/UH5KO65NqFJD2Rkssd8yL0Sv26SnkBLp+fQ09n6ngnRMOSyryX4weRBYFfoXgvsOtW0Jk8gWkRfGdmAdO2IPWQWbgizSSwvmQZJu5HmsFXAXmqw5U1Bo4AYgPmplYgYryMmirG2FzR/k8+f7w76DfNSMXYepRC+MVwExHXgRebxkS/kbG3oG6HqKRy7ajMGSIUvo0reDoxMilTnRfKkdJOBePDqkNX0CTyo2OnXvRTZq691egxZWLpj9eQ7M6mj4mcNWmQjraFNLpvE47AC/azlrhe0D9dyVfbSoyosCvEKRSbt0S4Alglf9ZGIhWMr0FmyeOiApf8UiqFl4GiZaauny5K840ZIocuu4udL23ex7Si+l3+c3u4EcrdhPrQHNQ6DUajxE+Z+fXZs5kV//DdJwjwCoa8CT0H+9G6l4dFNEu+zvSofJP42aMdM4CHa1creNWEkLvGZjOEv99K/HfY7j9osCvEGTP5fp96jueyGbvqAktf66BJmjGUAokEabQLp/MxPnCi/Gb9IpnrJcngHTr29gtWCuBh5j09esL0HglA/rffYBy0lR+vAv6DBsaN69DfQwmPc1W9Qh1tGsKsb2MPrGi5bgJyg2ZMcVURIFfKWQVQ0+7/NE+DHwdRhjHrWg8OoLyt0NXU5Fw5ci4pYzo7zwIH/HfuRXaeLOSgf3DMpic9BhuWA7Vo15fhI3Hwweu5o4HbmKW4+nUiItazh/kfnBLTEmwSqCHmaA5HwSFShFZyIhJ33TtGi4YaXKYf304zwTRzr9sxwXS3hTVpCf33mlHYaZQ7F0C1dYa/vixJ7eJAr9ikEHzNjGrs5xoq6mA4ZwTvsAaZYwPznMGVBy2IfdYK4oF1kuTPVtI/h+De5bAJ2NmYgoUr+iJm2GBzTHjhnBr8Gnx1kXyOtl1jHvAjPaj82L4hTJTJJJpq4RGRPnprkHtYWDuDuBMDxHt1cIdAzd9nQvQlkDxLNcUlUVYW05kUaGFRU8b0d1vheUq/SHKoHhZhcIiDTQ/l0MiML+a464Z2vx6ZAhkB8aXmeiF9XgLBiIFAVfno+agetrKWNDMRH8pMvfrt1GTxTfFCO3cDO+a4VglhZTcN8mafH4NvhQ4P6nFWx+OIkWuRsPbDVDN56KTN9qQU9LCuVFTSPU4JzAXxd1bgY3WV2Nfx46bbyciEnVqrTBLPzNxJUytnT9Ek16ER4ZCrQ7SU32xJzthOWQmSlH83jVUxqndVyqiwItMIdlUCb3KjI6zsWamLZPMROIDJ9RXrEBEH14kmPF+1JZ+jlxzl3BO/GLy0A5Dfgt++NuepZmstAwRNbxIMGnFOPbvhbA1EE0bs2m/ECzxzMRliqjhRUSSCFHDi4gkEaLAi4gkEaLAi4gkEaLAi4gkEaLAi4gkEaLAi4gkEaLAi4gkEaLAi4gkDcD/AyHHDnmYkAc7AAAAAElFTkSuQmCC)\n",
        "\n",
        "*  N is the total number of tags .\n",
        "*  C(t(i-1),t(i)) is the count of the tuple (previous POS, current POS) in transition_counts dictionary.\n",
        "* C(t(i-1)) is the count of the previous POS in the tag_counts dictionary.\n",
        "*  Î± is a smoothing parameter."
      ],
      "metadata": {
        "id": "-b7MjFisu-lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    ''' \n",
        "    Input: \n",
        "        alpha: number used for smoothing\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        transition_counts: transition count for the previous word and tag\n",
        "    Output:\n",
        "        A: matrix of dimension (num_tags,num_tags)\n",
        "    '''\n",
        "    # Get a sorted list of unique POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    \n",
        "    # Count the number of unique POS tags\n",
        "    num_tags = len(all_tags)\n",
        "    \n",
        "    # Initialize the transition matrix 'A'\n",
        "    A = np.zeros((num_tags,num_tags))\n",
        "    \n",
        "    # Get the unique transition tuples (previous POS, current POS)\n",
        "    trans_keys = set(transition_counts.keys())\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ### \n",
        "    \n",
        "    # Go through each row of the transition matrix A\n",
        "    for i in range(num_tags):\n",
        "        \n",
        "        # Go through each column of the transition matrix A\n",
        "        for j in range(num_tags):\n",
        "\n",
        "            # Initialize the count of the (prev POS, current POS) to zero\n",
        "            count = 0\n",
        "        \n",
        "            # Define the tuple (prev POS, current POS)\n",
        "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
        "            key = (all_tags[i], all_tags[j])\n",
        "\n",
        "            # Check if the (prev POS, current POS) tuple \n",
        "            # exists in the transition counts dictionary\n",
        "            if key in transition_counts.keys(): #complete this line\n",
        "                \n",
        "                # Get count from the transition_counts dictionary \n",
        "                # for the (prev POS, current POS) tuple\n",
        "                count = transition_counts[key]\n",
        "                \n",
        "            # Get the count of the previous tag (index position i) from tag_counts\n",
        "            count_prev_tag = tag_counts[all_tags[i]]\n",
        "            \n",
        "            # Apply smoothing using count of the tuple, alpha, \n",
        "            # count of previous tag, alpha, and total number of tags\n",
        "            A[i,j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return A"
      ],
      "metadata": {
        "id": "ZjEteOegwHRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.001\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "# Testing your function\n",
        "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
        "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
        "\n",
        "print(\"View a subset of transition matrix A\")\n",
        "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
        "print(A_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3t9JgWO-8Mv",
        "outputId": "ead8a48b-62a0-4cdb-a457-65159f70fd48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A at row 0, col 0: 0.000007040\n",
            "A at row 3, col 1: 0.1691\n",
            "View a subset of transition matrix A\n",
            "              RBS            RP           SYM        TO            UH\n",
            "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
            "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
            "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
            "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
            "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emission Probability table \n",
        "\n",
        "Now we would define the emissione prob table .\n",
        "Now you will create the B transition matrix which computes the emission probability. You will use smoothing as defined below:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "**The technique would be exactly the same as the transitiona matrix**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bnfnajuPC-uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        alpha: tuning parameter used in smoothing \n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index.\n",
        "               within the function it'll be treated as a list\n",
        "    Output:\n",
        "        B: a matrix of dimension (num_tags, len(vocab))\n",
        "    '''\n",
        "    \n",
        "    # get the number of POS tag\n",
        "    num_tags = len(tag_counts)\n",
        "    \n",
        "    # Get a list of all POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    \n",
        "    # Get the total number of unique words in the vocabulary\n",
        "    num_words = len(vocab)\n",
        "    \n",
        "    # Initialize the emission matrix B with places for\n",
        "    # tags in the rows and words in the columns\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "    \n",
        "    # Get a set of all (POS, word) tuples \n",
        "    # from the keys of the emission_counts dictionary\n",
        "    emis_keys = set(list(emission_counts.keys()))\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    \n",
        "    # Go through each row (POS tags)\n",
        "    for i in range(len(all_tags)): # complete this line\n",
        "        \n",
        "        # Go through each column (words)\n",
        "        for j in range(num_words): # complete this line\n",
        "\n",
        "            # Initialize the emission count for the (POS tag, word) to zero\n",
        "            count = 0\n",
        "                    \n",
        "            # Define the (POS tag, word) tuple for this row and column\n",
        "            key =  (all_tags[i],vocab[j])\n",
        "\n",
        "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
        "            if emission_counts.keys(): # complete this line\n",
        "        \n",
        "                # Get the count of (POS tag, word) from the emission_counts d\n",
        "                count = emission_counts[key]\n",
        "                \n",
        "            # Get the count of the POS tag\n",
        "            count_tag = tag_counts[all_tags[i]]\n",
        "                \n",
        "            # Apply smoothing and store the smoothed value \n",
        "            # into the emission matrix B for this row and column\n",
        "            B[i,j] = (count + alpha) / (count_tag + alpha*num_tags)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return B"
      ],
      "metadata": {
        "id": "glaVp_KaG4yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
        "\n",
        "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
        "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
        "\n",
        "# Try viewing emissions for a few words in a sample dataframe\n",
        "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
        "\n",
        "# Get the integer ID for each word\n",
        "cols = [vocab[a] for a in cidx]\n",
        "\n",
        "# Choose POS tags to show in a sample dataframe\n",
        "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
        "\n",
        "# For each POS tag, get the row number from the 'states' list\n",
        "rows = [states.index(a) for a in rvals]\n",
        "\n",
        "# Get the emissions for the sample of words, and the sample of POS tags\n",
        "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
        "print(B_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruDxoNmWJHh0",
        "outputId": "c2c6dd39-5364-40e8-9b4c-f9796a5a1da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View Matrix position at row 0, column 0: 0.000007040\n",
            "View Matrix position at row 3, column 1: 0.000000732\n",
            "              725      adroitly     engineers      promoted       synergy\n",
            "CD   8.206618e-05  2.734628e-08  2.734628e-08  2.734628e-08  2.734628e-08\n",
            "NN   7.522471e-09  7.522471e-09  7.522471e-09  7.522471e-09  2.257493e-05\n",
            "NNS  1.670675e-08  1.670675e-08  4.678057e-04  1.670675e-08  1.670675e-08\n",
            "VB   3.782428e-08  3.782428e-08  3.782428e-08  3.782428e-08  3.782428e-08\n",
            "RB   3.228926e-08  6.461082e-05  3.228926e-08  3.228926e-08  3.228926e-08\n",
            "RP   3.756509e-07  3.756509e-07  3.756509e-07  3.756509e-07  3.756509e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viterbi Algorithm ( using Dynamic Programming )\n",
        "\n",
        "In this section we woudl implementing the Viterbi Algorithm . There are three main components to implementing the Viterbi algorithm , \n",
        " * Initialization .\n",
        " * Forward Pass. \n",
        " * Backward Pass.\n",
        "\n",
        "## Initialization \n",
        "We would start by initialization  of the matric \n",
        "\n",
        "**best_probs**: Each cell contains the probability of going from one POS tag to a word in the corpus.\n",
        "\n",
        "**best_paths**: A matrix that helps you trace through the best possible path in the corpus."
      ],
      "metadata": {
        "id": "1OGs2ZYmJ957"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        states: a list of all possible parts-of-speech\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
        "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
        "        corpus: a sequence of words whose POS is to be identified in a list \n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
        "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
        "    '''\n",
        "    # Get the total number of unique POS tags\n",
        "    num_tags = len(tag_counts)\n",
        "    \n",
        "    # Initialize best_probs matrix \n",
        "    # POS tags in the rows, number of words in the corpus as the columns\n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\n",
        "    \n",
        "    # Initialize best_paths matrix\n",
        "    # POS tags in the rows, number of words in the corpus as columns\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
        "    \n",
        "    # Define the start token\n",
        "    s_idx = states.index(\"--s--\")\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    \n",
        "    # Go through each of the POS tags\n",
        "    for i in range(num_tags): # complete this line\n",
        "        \n",
        "        # Handle the special case when the transition from start token to POS tag i is zero\n",
        "        if A[s_idx,i] == 0: # complete this line\n",
        "            \n",
        "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
        "            best_probs[i,0] = -23454566788\n",
        "        \n",
        "        # For all other cases when transition from start token to POS tag i is non-zero:\n",
        "        else:\n",
        "            \n",
        "            # Initialize best_probs at POS tag 'i', column 0\n",
        "            # Check the formula in the instructions above\n",
        "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]])\n",
        "\n",
        "                        \n",
        "    ### END CODE HERE ### \n",
        "    return best_probs, best_paths"
      ],
      "metadata": {
        "id": "_sGWJqJgJ8Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
      ],
      "metadata": {
        "id": "W0oP4y3ejIC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
        "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygtfV9DTjJoK",
        "outputId": "1fcb0227-574a-45f5-afab-36baf480fa7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_probs[0,0]: -22.4553\n",
            "best_paths[2,3]: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viterbi Forward Pass \n",
        "\n",
        "Now we would implement the forward pass of the Viterbi algorithms \n",
        "Following are the steps involved in the algorithms \n",
        "\n",
        "\n",
        "*   Walk forward through the corpus.\n",
        "*   For each word, compute a probability for each possible tag.\n",
        "*   Unlike the previous algorithm predict_pos (the 'warm-up' exercise), this will include the path up to that (word,tag) combination.\n",
        "\n"
      ],
      "metadata": {
        "id": "aKosbb-bm04T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        A, B: The transition and emission matrices respectively\n",
        "        test_corpus: a list containing a preprocessed corpus\n",
        "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
        "    Output: \n",
        "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
        "    '''\n",
        "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
        "    num_tags = best_probs.shape[0]\n",
        "    \n",
        "    # Go through every word in the corpus starting from word 1\n",
        "    # Recall that word 0 was initialized in `initialize()`\n",
        "    for i in range(1, len(test_corpus)): \n",
        "        \n",
        "        # Print number of words processed, every 5000 words\n",
        "        if i % 5000 == 0:\n",
        "            print(\"Words processed: {:>8}\".format(i))\n",
        "            \n",
        "        ### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###\n",
        "        # For each unique POS tag that the current word can be\n",
        "        for j in range(num_tags): # complete this line\n",
        "            \n",
        "            # Initialize best_prob for word i to negative infinity\n",
        "            best_prob_i = float(\"-inf\")\n",
        "            \n",
        "            # Initialize best_path for current word i to None\n",
        "            best_path_i = None\n",
        "\n",
        "            # For each POS tag that the previous word can be:\n",
        "            for k in range(num_tags): # complete this line\n",
        "            \n",
        "                # Calculate the probability = \n",
        "                # best probs of POS tag k, previous word i-1 + \n",
        "                # log(prob of transition from POS k to POS j) + \n",
        "                # log(prob that emission of POS j is word i)\n",
        "                prob = best_probs[k][i-1] + np.log(A[k][j]) + np.log(B[j][vocab[test_corpus[i]]])\n",
        "\n",
        "                # check if this path's probability is greater than\n",
        "                # the best probability up to and before this point\n",
        "                if prob > best_prob_i: # complete this line\n",
        "                    \n",
        "                    # Keep track of the best probability\n",
        "                    best_prob_i = prob\n",
        "                    \n",
        "                    # keep track of the POS tag of the previous word\n",
        "                    # that is part of the best path.  \n",
        "                    # Save the index (integer) associated with \n",
        "                    # that previous word's POS tag\n",
        "                    best_path_i = k\n",
        "\n",
        "            # Save the best probability for the \n",
        "            # given current word's POS tag\n",
        "            # and the position of the current word inside the corpus\n",
        "            best_probs[j,i] = best_prob_i\n",
        "            \n",
        "            # Save the unique integer ID of the previous POS tag\n",
        "            # into best_paths matrix, for the POS tag of the current word\n",
        "            # and the position of the current word inside the corpus.\n",
        "            best_paths[j,i] = best_path_i\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "    return best_probs, best_paths"
      ],
      "metadata": {
        "id": "GRDzE-_nmzxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN1gBL_jZ6BP",
        "outputId": "f047fa2c-38d0-45d8-c7e9-859dae5f075d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words processed:     5000\n",
            "Words processed:    10000\n",
            "Words processed:    15000\n",
            "Words processed:    20000\n",
            "Words processed:    25000\n",
            "Words processed:    30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the backward pass here \n",
        "\n",
        "* This would allow us to generate the probability values for the words based on the sequences of data .\n"
      ],
      "metadata": {
        "id": "mDgSVVxefEHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
        "    '''\n",
        "    This function returns the best path.\n",
        "    \n",
        "    '''\n",
        "    # Get the number of words in the corpus\n",
        "    # which is also the number of columns in best_probs, best_paths\n",
        "    m = best_paths.shape[1] \n",
        "    \n",
        "    # Initialize array z, same length as the corpus\n",
        "    z = [None] * m\n",
        "    \n",
        "    # Get the number of unique POS tags\n",
        "    num_tags = best_probs.shape[0]\n",
        "    \n",
        "    # Initialize the best probability for the last word\n",
        "    best_prob_for_last_word = float('-inf')\n",
        "    \n",
        "    # Initialize pred array, same length as corpus\n",
        "    pred = [None] * m\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    ## Step 1 ##\n",
        "    \n",
        "    # Go through each POS tag for the last word (last column of best_probs)\n",
        "    # in order to find the row (POS tag integer ID) \n",
        "    # with highest probability for the last word\n",
        "    for k in range(num_tags): # complete this line\n",
        "\n",
        "        # If the probability of POS tag at row k \n",
        "        # is better than the previously best probability for the last word:\n",
        "        if best_probs[k,-1] > best_prob_for_last_word: # complete this line\n",
        "            \n",
        "            # Store the new best probability for the lsat word\n",
        "            best_prob_for_last_word = best_probs[k,-1]\n",
        "    \n",
        "            # Store the unique integer ID of the POS tag\n",
        "            # which is also the row number in best_probs\n",
        "            z[m - 1] = k\n",
        "            \n",
        "    # Convert the last word's predicted POS tag\n",
        "    # from its unique integer ID into the string representation\n",
        "    # using the 'states' dictionary\n",
        "    # store this in the 'pred' array for the last word\n",
        "    pred[m - 1] = states[k]\n",
        "    \n",
        "    ## Step 2 ##\n",
        "    # Find the best POS tags by walking backward through the best_paths\n",
        "    # From the last word in the corpus to the 0th word in the corpus\n",
        "    for i in range(len(corpus)-1, -1, -1): # complete this line\n",
        "        \n",
        "        # Retrieve the unique integer ID of\n",
        "        # the POS tag for the word at position 'i' in the corpus\n",
        "        pos_tag_for_word_i = best_paths[np.argmax(best_probs[:,i]),i]\n",
        "        \n",
        "        # In best_paths, go to the row representing the POS tag of word i\n",
        "        # and the column representing the word's position in the corpus\n",
        "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
        "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\n",
        "        \n",
        "        # Get the previous word's POS tag in string form\n",
        "        # Use the 'states' dictionary, \n",
        "        # where the key is the unique integer ID of the POS tag,\n",
        "        # and the value is the string representation of that POS tag\n",
        "        pred[i - 1] = states[pos_tag_for_word_i]\n",
        "        \n",
        "     ### END CODE HERE ###\n",
        "    return pred"
      ],
      "metadata": {
        "id": "BmOW5FyUfDVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run and test your function\n",
        "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
        "m=len(pred)\n",
        "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
        "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taNTSP6-kuJp",
        "outputId": "0048c793-fb22-4d9b-e5e9-62241e3ed8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction for pred[-7:m-1] is: \n",
            " ['see', 'them', 'here', 'with', 'us', '.'] \n",
            " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
            "\n",
            "The prediction for pred[0:8] is: \n",
            " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
            " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting on the data set\n",
        "\n",
        "Now  we would be predicting the accuracy of the model using our HMM model .\n"
      ],
      "metadata": {
        "id": "HDcA90uHcIBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(pred, y):\n",
        "    '''\n",
        "    Input: \n",
        "        pred: a list of the predicted parts-of-speech \n",
        "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
        "    Output: \n",
        "        \n",
        "    '''\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Zip together the prediction and the labels\n",
        "    for prediction, y in zip(pred, y):\n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "        # Split the label into the word and the POS tag\n",
        "        word_tag_tuple = y.split()\n",
        "        \n",
        "        # Check that there is actually a word and a tag\n",
        "        # no more and no less than 2 items\n",
        "        if len(word_tag_tuple)!=2: # complete this line\n",
        "            continue \n",
        "\n",
        "        # store the word and tag separately\n",
        "        word, tag = word_tag_tuple\n",
        "        \n",
        "        # Check if the POS tag label matches the prediction\n",
        "        if prediction == tag: # complete this line\n",
        "            \n",
        "            # count the number of times that the prediction\n",
        "            # and label match\n",
        "            num_correct += 1\n",
        "            \n",
        "        # keep track of the total number of examples (that have valid labels)\n",
        "        total += 1\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "    return num_correct/total"
      ],
      "metadata": {
        "id": "9kWdNOgaex20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4za2EFAe09r",
        "outputId": "6ded2979-333f-4f73-ba5d-7b44d80a8eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Viterbi algorithm is 0.9526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion \n",
        "\n",
        "Now we can see how we can improve the model accuracy by using the HMM model in contrast to simple frequency model , based on basic probabilistic techniques."
      ],
      "metadata": {
        "id": "hqyXaumImISA"
      }
    }
  ]
}